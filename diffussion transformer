import tensorflow as tf
from tensorflow.keras import layers
import math

# ---- Linformer Self-Attention for Sequences ----
class LinformerAttention(layers.Layer):
    def __init__(self, model_dim, n_heads, k):
        super().__init__()
        self.n_heads = n_heads
        self.model_dim = model_dim
        self.head_dim = model_dim // n_heads

        self.wq = layers.Dense(model_dim)
        self.wk = layers.Dense(model_dim)
        self.wv = layers.Dense(model_dim)
        self.E = layers.Dense(k)
        self.F = layers.Dense(k)
        self.wo = layers.Dense(model_dim)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.head_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, q, k, v):
        batch_size = tf.shape(q)[0]
        q, k, v = self.wq(q), self.wk(k), self.wv(v)
        q, k, v = self.split_heads(q, batch_size), self.split_heads(k, batch_size), self.split_heads(v, batch_size)
        k, v = tf.transpose(self.E(tf.transpose(k, [0,1,3,2])), [0,1,3,2]), tf.transpose(self.F(tf.transpose(v, [0,1,3,2])), [0,1,3,2])
        scaled_attention = tf.nn.softmax(tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(float(self.head_dim)), axis=-1)
        output = tf.matmul(scaled_attention, v)
        output = tf.reshape(tf.transpose(output, [0,2,1,3]), (batch_size, -1, self.model_dim))
        return self.wo(output)

# ---- Transformer Block ----
class DiTBlock(layers.Layer):
    def __init__(self, model_dim, n_heads, mlp_dim, k):
        super().__init__()
        self.attn = LinformerAttention(model_dim, n_heads, k)
        self.norm1 = layers.LayerNormalization()
        self.norm2 = layers.LayerNormalization()
        self.mlp = tf.keras.Sequential([
            layers.Dense(mlp_dim, activation='gelu'),
            layers.Dense(model_dim)
        ])

    def call(self, x):
        attn_out = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))
        x = x + attn_out
        x = x + self.mlp(self.norm2(x))
        return x

# ---- Main Diffusion Transformer Model for Sequences ----
class GeneExpressionTransformer(tf.keras.Model):
    def __init__(self, seq_len=50000, model_dim=128, depth=4, heads=4, mlp_dim=256, k=256, output_dim=1):
        super().__init__()
        self.embedding = layers.Embedding(input_dim=4, output_dim=model_dim)  # 4 nucleotide tokens: A, T, C, G
        self.pos_embed = layers.Embedding(input_dim=seq_len, output_dim=model_dim)
        self.transformer_blocks = [DiTBlock(model_dim, heads, mlp_dim, k) for _ in range(depth)]
        self.final_dense = layers.Dense(output_dim)

    def call(self, x):  # x shape: [batch, seq_len]
        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)
        x = self.embedding(x) + self.pos_embed(positions)
        for block in self.transformer_blocks:
            x = block(x)
        return self.final_dense(tf.reduce_mean(x, axis=1))  # [batch, output_dim]



#use model like 
model = GeneExpressionTransformer(seq_len=50000)
output = model(input_tensor)  # input_tensor shape: [batch, 50000]

